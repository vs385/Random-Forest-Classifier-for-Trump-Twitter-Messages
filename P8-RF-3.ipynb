{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from  sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCols(columns, data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the data frame with columns removed\n",
    "    \n",
    "    input\n",
    "    ------\n",
    "    columns: list of column names\n",
    "    data: pandas dataframe\n",
    "    \"\"\"\n",
    "    data = data.drop(columns, axis=1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can drop replyToSID, latitude, longitude, screenName\n",
    "to_drop = [\"replyToSID\", \"latitude\", \"longitude\", \"screenName\", \"id.1\", \"statusSource\"]\n",
    "data = dropCols(to_drop, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = data.shape\n",
    "n,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates new fields\n",
    "\n",
    "data[\"has_link\"] = -1\n",
    "data[\"is_capitalized\"] = -1\n",
    "data[\"has_I\"] = -1\n",
    "data[\"has_Crooked\"] = -1\n",
    "data[\"has_TY\"] = -1\n",
    "data[\"has_Hillary\"] = -1\n",
    "\n",
    "hour = []\n",
    "\n",
    "for i in range(len(data[\"text\"])):\n",
    "    if \"https\" in data[\"text\"][i]:\n",
    "        data[\"has_link\"][i] = 1\n",
    "        \n",
    "    if (data[\"text\"][i]).isupper():\n",
    "        data[\"is_capitalized\"][i] = 1\n",
    "        \n",
    "    if \"I \" in data[\"text\"][i]:\n",
    "        data[\"has_I\"][i] = 1\n",
    "        \n",
    "    if \"rooked\" in data[\"text\"][i]:\n",
    "        data[\"has_Crooked\"][i] = 1\n",
    "    \n",
    "    if \"Thank you\" in data[\"text\"][i]:\n",
    "        data[\"has_TY\"][i] = 1\n",
    "        \n",
    "    if \"Hillary\" in data[\"text\"][i]:\n",
    "        data[\"has_Hillary\"][i] = 1\n",
    "        \n",
    "    h_idx = data[\"created\"][i].split()[1].rfind(\":\")\n",
    "    hour.append(int(data[\"created\"][i].split()[1][:h_idx]))\n",
    "    \n",
    "data[\"hour\"] = hour\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 1 is Android(Trump) and -1 is iphone(not Trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the data into train/val with ration 0.8:0.2\n",
    "\n",
    "yTr = data[\"label\"]\n",
    "xTr = data.loc[:, data.columns != 'label']\n",
    "xTr, xVal, yTr, yVal = train_test_split(xTr, yTr, test_size=0.2, random_state=500)\n",
    "id_tr = xTr[\"id\"]\n",
    "id_val = xVal[\"id\"]\n",
    "xTr.drop(\"id\", axis=1, inplace = True)\n",
    "xVal.drop(\"id\", axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: Sena's written function from CS 4300\n",
    "\n",
    "features = 5000\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object\n",
    "    \n",
    "    Params: {max_features: Integer,\n",
    "             max_df: Float,\n",
    "             min_df: Float,\n",
    "             norm: String,\n",
    "             stop_words: String}\n",
    "    Returns: TfidfVectorizer\n",
    "    \"\"\"\n",
    "    \n",
    "    tfidfvec = TfidfVectorizer(max_features = max_features, stop_words = stop_words, \\\n",
    "                    max_df = max_df, min_df = min_df, norm=norm)\n",
    "    return tfidfvec\n",
    "    \n",
    "tfidf_vec = build_vectorizer(features, \"english\")\n",
    "train_doc = tfidf_vec.fit_transform([d for d in xTr.text]).toarray()\n",
    "val_doc = tfidf_vec.transform([d for d in xVal.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline model uses a Multinomial Naive Bayes classifier\n",
    "\n",
    "clf = MultinomialNB().fit(train_doc, yTr)\n",
    "predicted = clf.predict(val_doc)\n",
    "print(classification_report(yVal,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in the Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop2 = [\"replyToSID\", \"latitude\", \"longitude\", \"screenName\", \"id.1\"]\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data = dropCols(to_drop2, test_data)\n",
    "test_data_id = test_data[\"id\"]\n",
    "test_data = test_data.loc[:, test_data.columns != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates 2 new fields: 1 for if the text has a tweet and the other if the entire text is capitalized\n",
    "\n",
    "test_data[\"has_link\"] = -1\n",
    "test_data[\"is_capitalized\"] = -1\n",
    "test_data[\"has_I\"] = -1\n",
    "test_data[\"has_Crooked\"] = -1\n",
    "test_data[\"has_TY\"] = -1\n",
    "test_data[\"has_Hillary\"] = -1\n",
    "\n",
    "t_hour = []\n",
    "\n",
    "for i in range(len(test_data[\"text\"])):\n",
    "    if \"https\" in test_data[\"text\"][i]:\n",
    "        test_data[\"has_link\"][i] = 1\n",
    "        \n",
    "    if (test_data[\"text\"][i]).isupper():\n",
    "        test_data[\"is_capitalized\"][i] = 1\n",
    "        \n",
    "    if \"I \" in test_data[\"text\"][i]:\n",
    "        test_data[\"has_I\"][i] = 1\n",
    "        \n",
    "    if \"rooked\" in test_data[\"text\"][i]:\n",
    "        test_data[\"has_Crooked\"][i] = 1\n",
    "        \n",
    "    if \"Thank you\" in data[\"text\"][i]:\n",
    "        test_data[\"has_TY\"][i] = 1        \n",
    "        \n",
    "    if \"Hillary\" in data[\"text\"][i]:\n",
    "        test_data[\"has_Hillary\"][i] = 1\n",
    "        \n",
    "    h_idx = test_data[\"created\"][i].split()[1].rfind(\":\")\n",
    "    t_hour.append(int(test_data[\"created\"][i].split()[1][:h_idx]))\n",
    "        \n",
    "test_data[\"hour\"] = t_hour\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = tfidf_vec.transform([d for d in test_data.text])\n",
    "prediction_test = clf.predict(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes to csv.\n",
    "#NB: you need to open CSV file and delete the first column and save before submitting to Kaggle\n",
    "\n",
    "cols = {\"ID\":test_data_id, \"Label\": prediction_test}\n",
    "df = pd.DataFrame(cols)\n",
    "df.to_csv(\"MultiNB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 vector representation of textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def tokenizeText(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns tokenized text\n",
    "    \n",
    "    input\n",
    "    ------\n",
    "    text: list of tweets\n",
    "    \n",
    "    output\n",
    "    -------\n",
    "    tokenized_text: list of tokenized words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_text = []\n",
    "    \n",
    "    for row in text:\n",
    "        words = tokenizer.tokenize(row)\n",
    "        \n",
    "        #remove stop words before appending\n",
    "        tokenized_text.append([word for word in words if word not in stopwords.words('english')])\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "tokenized_text_train = tokenizeText(xTr.text)\n",
    "tokenized_text_test = tokenizeText(xVal.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the w2v model\n",
    "model = Word2Vec(tokenized_text_train, size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgVec(model, tokenized_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a vector representation for all sentences\n",
    "    \n",
    "    input\n",
    "    ------\n",
    "    model: trained word2vec model\n",
    "    tokenized_text: list of tokenized text\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    avg_vec_train: numpy array where vectors on each row represent each sentence in train set\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_vec_train = np.zeros((len(tokenized_text),300))\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "\n",
    "        vec = np.zeros((1,300))\n",
    "        for item in tokenized_text[i]:\n",
    "            if item not in model:\n",
    "                continue\n",
    "            else:\n",
    "                vec += model.wv[item]\n",
    "\n",
    "        vec /= len(tokenized_text[i])\n",
    "        \n",
    "        avg_vec_train[i, :] = vec\n",
    "    \n",
    "    return avg_vec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_train = getAvgVec(model, tokenized_text_train)\n",
    "vecs_val = getAvgVec(model, tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert true/fale to 0/1\n",
    "for col in xTr.columns:\n",
    "    if (xTr[col]).dtype == \"bool\":\n",
    "        xTr[col] *= 1\n",
    "        xVal[col] *= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateColsAndArray(data, nparray, colsToRemove=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a numpy array of w2v concatenated with other columns\n",
    "    \n",
    "    input\n",
    "    -------\n",
    "    data: dataframe\n",
    "    nparray: array of vecs for sentences\n",
    "    colsToRemove: list of columns to exclude from dataframe\n",
    "    \n",
    "    output\n",
    "    -------\n",
    "    newdata: numpy array representation of data\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(colsToRemove) !=0:\n",
    "        newdata = data[data.columns.difference(colsToRemove)]\n",
    "        newdata = np.concatenate((nparray, newdata.values), axis=1)\n",
    "    else:\n",
    "        newdata = np.concatenate((nparray, data.values), axis=1)\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate other columns with array\n",
    "cols_to_remove = ['created', 'replyToSN', \"replyToUID\", \"text\"]\n",
    "rf_input_train = concatenateColsAndArray(xTr, vecs_train,cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFClassifier(ntrees, nfeatures):\n",
    "    \"\"\"\n",
    "    Builds Random Forest Classifier\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=ntrees, \n",
    "                                   #max_features=np.round(np.sqrt(nfeatures)),\n",
    "                                  max_depth=30)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees = 5000\n",
    "nfeatures = 300\n",
    "RFC = RFClassifier(ntrees, nfeatures)\n",
    "forest = RFC.fit(rf_input_train, yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_val = xVal[xVal.columns.difference(['created', 'replyToSN', \"replyToUID\", \"text\"])]\n",
    "rf_input_val = concatenateColsAndArray(xVal, vecs_val,cols_to_remove)\n",
    "\n",
    "#np.concatenate((vecs_val, RF_val.values), axis=1)\n",
    "res = forest.predict(rf_input_val)\n",
    "print(classification_report(yVal,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizeText(test_data.text)\n",
    "vecs_test = getAvgVec(model, tokenized_test)\n",
    "rf_input_test = concatenateColsAndArray(test_data, vecs_test ,cols_to_remove)\n",
    "test_res = forest.predict(rf_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes to csv\n",
    "\n",
    "cols = {\"ID\":test_data_id, \"Label\": test_res}\n",
    "df = pd.DataFrame(cols)\n",
    "df.to_csv(\"RFtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
